{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee631914",
   "metadata": {},
   "source": [
    "# Workshop 8\n",
    "\n",
    "The purpose of this workshop is to expose you to concepts related to maximum log likelihood estimation. This should also help you to learn about numpy, scipy methods. Go through these exercises will also help you with your homework 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712c3b82",
   "metadata": {},
   "source": [
    "# 1. Maximum log likelihood estimator\n",
    "\n",
    "In a single bin counting experiment, we measure an observed outcome $k$ that follows a Poisson distribution with an expectation of $\\lambda$. The likelihood function of this measurement is simply Poisson($k|\\lambda$). The expectation is parameterized as $\\lambda = \\mu\\cdot s + b$ where $s$ and $b$ are the expected number of signal and expected number of background evnets, respectively, and $\\mu$ is the so-called signal strength parameter that can scales up and down the expected signal contribution in the $\\lambda$. In the measurement, $\\mu$ is the parameter of interest that we want to determine from the observation, and $s$ and $b$ are constants. \n",
    "\n",
    "In a maximum log likelihood fit, we would need to maximize the value of the log likelihood function of the measurement. Equivally, we would minimize the **negative log likelihood (NLL)** function of the measurement, which is given as \n",
    "\n",
    "$$\n",
    "-\\mathrm{log}L(k|\\lambda) = -\\lambda + k\\mathrm{log}(\\lambda) - \\mathrm{log}(k!),\n",
    "$$\n",
    "where $\\lambda = \\mu\\cdot s + b$.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e222af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8b7d1",
   "metadata": {},
   "source": [
    "Task 1: define a function to implement this single bin negative log likelihood function. The function should have four input arguments, $\\mu$, $k$, $s$, and $b$. Hint: this can be easily done with the `logpmf` method of the scipy.stats.poisson\n",
    "\n",
    "You may name this function as NLP for the sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c38793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP(mu, obs, sig, bkg):\n",
    "    # develop your code here\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f54a3b6",
   "metadata": {},
   "source": [
    "Set $s$ = 5, $b$ = 100, and $k$ = 115. Show the `NLP` as a function of $\\mu$. The sequence of $\\mu$ values should be given by `mu = np.linspace(1,5,401)`. If your NLP is defined correctly, your plot should look like this one.\n",
    "\n",
    "<img src='https://portal.nersc.gov/project/m3438/physics77/WS08/fig1.png' width=500>\n",
    "\n",
    "\n",
    "The $\\mu$ value that minimizes the negative log likelihood function is our measured $\\mu$. What is it? \n",
    "\n",
    "Hint - you may use np.argmin() to find the position of the entry with the minimum value, and then use this position to determine the $\\mu$ value that minimizes the negative log likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8eb917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60809af",
   "metadata": {},
   "source": [
    "### scipy.optimize.minimize\n",
    "\n",
    "Now we use the scipy.optimize.minimize method to determine the $\\mu$ value. Let's look at the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f69d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for minimization\n",
    "def func(x,a,b,c):\n",
    "    return (x[0]-a)**2 + (x[1]-b)**2 + c\n",
    "\n",
    "# the minimum of this function should be c when x = [a,b] \n",
    "\n",
    "# x is a list or numpy array (preferred)\n",
    "# if your function has N free parameters to be determined by the minimization\n",
    "# then x should have a shape of (N,) or x should be a list with N entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a526922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = minimize(func,x0=[0,0], args=(3,4,20))\n",
    "\n",
    "#This line minimizes the function of \"func\"\n",
    "# args sets the constant parameters of func, \n",
    "#in this case we have a = 3, b = 4, c = 20\n",
    "# the free parameters in func is given by x\n",
    "# x0=[0,0] sets some initial guess of these values\n",
    "# these values are determined by the minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997efab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result of the minimization is saved in the object call result\n",
    "# Let's print it out\n",
    "print(result)\n",
    "\n",
    "# \"fun\" is the minimized value of the functionh\n",
    "# x is the array storing the values that minimize the function\n",
    "# is x matching x = [a,b]?\n",
    "# is the minimized value the same as c?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# to retrive values from the returning object of the minimize method\n",
    "# simply do\n",
    "\n",
    "print(result.fun)\n",
    "print(result.x)\n",
    "print(result.x[0])\n",
    "print(result.x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b19c1a2",
   "metadata": {},
   "source": [
    "### Now, use the scipy.optimize.minimize to determine the $\\mu$ value that minimizes the negative log likelihood of this measurement\n",
    "\n",
    "- recall the $k$, $s$, and $b$ should be constants in the NLP function\n",
    "- $\\mu$ is the single free parameter in the fit\n",
    "\n",
    "How does this compare with the $\\mu$ value you determined by scanning the NLP as a function of $\\mu$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49e5aeb",
   "metadata": {},
   "source": [
    "## Functions with multiple minima\n",
    "\n",
    "While minimize is a great tool, sometimes it is still intuitive to perform the scan of the negative log likelihood. \n",
    "\n",
    "Now, consider the expectation is parameterized as $$ \\lambda = \\kappa^{2} s + b$$\n",
    "\n",
    "where $\\kappa$ is the parameter we want to measure. In this case, our negative log likelihood function will depend on $\\kappa$ and it is given in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea71d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell requires you define a NLP function in the earlier part of this workshop\n",
    "\n",
    "def NLP_vs_kappa( kappa, obs, sig, bkg ):\n",
    "    return NLP( kappa**2, obs, sig, bkg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3cc03c",
   "metadata": {},
   "source": [
    "Let's visualize the negative log likelihood function's dependence on $\\kappa$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b983b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = np.linspace(-3,3,6001)\n",
    "NLP_kappascan=NLP_vs_kappa(kappa,obs,sig,bkg)\n",
    "plt.plot(kappa,NLP_kappascan)\n",
    "plt.xlabel('$\\kappa$')\n",
    "plt.ylabel('Negative Log Likelihood Function')\n",
    "plt.text(-2,5,'Obs = 110, s = 5, b = 100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64334343",
   "metadata": {},
   "source": [
    "**Write your code below to determine the $\\kappa$ values that minimize the negative log likelihood function. Note that there are two minima in this function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34129192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9354eee",
   "metadata": {},
   "source": [
    "#### Now let's see what would this line with scipy.optimize.minimize find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe56f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = minimize(NLP_vs_kappa, x0=np.ones(1), args=(obs,sig,bkg), method='Nelder-Mead' )\n",
    "print(result_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c064eaa2",
   "metadata": {},
   "source": [
    "#### How about this one?\n",
    "\n",
    "*Are these fit results different? Why? What is different between this line and the one above?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3 = minimize(NLP_vs_kappa, x0=np.ones(1)*(-1), args=(obs,sig,bkg), method='Nelder-Mead' )\n",
    "print(result_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ec1073",
   "metadata": {},
   "source": [
    "You may have also noticed that in the minimization function, an option of method is specified. You can see a full list of avaialble methods here\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
    "These are different ways to converge on the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2160ea76",
   "metadata": {},
   "source": [
    "## local and global minima\n",
    "\n",
    "Now we switch to a slightly more complicated parameterization of $\\lambda$. We have \n",
    "$$ \\lambda = (-0.5\\kappa+1)\\kappa^{2}\\cdot s + b \n",
    "$$\n",
    "    This parameterization would give you two minima in the NLL curve. The one that has smaller value is called a `global minimum`, and the one that has a larger value is called a `local minimum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c58b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell requires you define a NLP function in the earlier part of this workshop\n",
    "def NLP_vs_kappa_v2( kappa, obs, sig, bkg ):\n",
    "    return NLP( (-0.5*kappa+1)*(kappa)**2 , obs, sig, bkg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785ec5df",
   "metadata": {},
   "source": [
    "**visualize the negative log likelihood function's dependence on $\\kappa$. Your plot should look like**\n",
    "\n",
    "<img src=\"https://portal.nersc.gov/project/m3438/physics77/WS08/fig2.png\" width =500>\n",
    "\n",
    "This should indicates that the global minimum is around $\\kappa$ ~ -1.4, but there is a local minimum at around +1.4 in $\\kappa$.  \n",
    "\n",
    "**Also determine the minimum of the negative log likelihood function in the range of -3 < $\\kappa$ < +3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b99aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "# plot\n",
    "\n",
    "\n",
    "\n",
    "#minimum \n",
    "print('The kappa value that minimizes the negative log likelihood is {:4.4f}'.format())\n",
    "print('The minimum of NLL is {:4.4f} '.format()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5403b39",
   "metadata": {},
   "source": [
    "**Develop a minimization code to determine the $\\kappa$ (kappa) value that minimizes the NLL**\n",
    "- Try setting the initial value of $\\kappa$ to + 1 and - 1 by changing the `x0=` option and see if the minization result is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_4 = minimize(NLP_vs_kappa_v2, x0=np.ones(1), args=(obs,sig,bkg), method='Nelder-Mead' )\n",
    "print(result_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55fa118",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_5 = minimize(NLP_vs_kappa_v2, x0=-np.ones(1), args=(obs,sig,bkg), method='Nelder-Mead' )\n",
    "print(result_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d5067",
   "metadata": {},
   "source": [
    "You will see if the initial value of $\\kappa$ is positive, then the minimization is trapped in the local minimum. This speaks to the importance of properly setting the initial value as well as the need to develop robust minimization methods. \n",
    "\n",
    "### Basinhopping\n",
    "Below is another minimization method provided in scipy.optimize, which has strategies to get out of the local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb523f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import basinhopping \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ef3c32",
   "metadata": {},
   "source": [
    "Duplicate the lines below into new cells, and try different stepsize (0.1, 1, 2 ) and different niter (50,500). **Are there any particular configurations that allow you to find the global minimum?**\n",
    "\n",
    "Consult this page to understand the meaning of stepsize and niter\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.basinhopping.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a29d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_basinhopping=basinhopping(NLP_vs_kappa_v2, x0=np.ones(1), stepsize=0.2, niter=120, minimizer_kwargs={\"args\":(obs,sig,bkg)})\n",
    "print(result_basinhopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a different parameter configuration here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4bf4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a different parameter configuration here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74779af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a different parameter configuration here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e73feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a different parameter configuration here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b9918",
   "metadata": {},
   "source": [
    "# 2. Fit\n",
    "\n",
    "For a given function with a set of tunable parameters, we can perform a fit to data to determine the set of parameter values that makes the function best describe data. We will see a few different ways to fit data in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f1af3",
   "metadata": {},
   "source": [
    "## Generate a sample of normally distributed random numbers\n",
    "\n",
    "**Use numpy to generate a sample of random numbers that follow a Gaussian distribution with a mean of 125 and a standard devation of 2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b3e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "# declare a random number generator\n",
    "rng = \n",
    "\n",
    "# generate the random set\n",
    "data = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c1daf",
   "metadata": {},
   "source": [
    "## rv_continuous fit\n",
    "In scipy.stats, if a function is continuously defined, meaning that its input variable is continuous rather than discrete, then it has a fit method. For example, a Gaussian function is such a function. scipy.stats use \"rv_continuous\" to refer to such functions. \n",
    "\n",
    "The default approach used here is a maximum log likelihood method, in other words, a log likelihood function is constructed, then values of parameters that maximize the log likelihood function are returned. However, the rv_continuous does not return the maximized log likelihood values. Nonetheless, one can easily construct it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97f268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to import norm from scipy.stats first\n",
    "# norm represents normal distribution \n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f08d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the fit is done by a single line\n",
    "# the returning arguments are the mean and the standard deviation of the fitted function\n",
    "# see https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html for more\n",
    "mean, std = norm.fit(data)\n",
    "\n",
    "print(mean,std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d42ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To visualize the fit result\n",
    "\n",
    "# Plot the random numbers\n",
    "bincontent, binedge, others = plt.hist(data,bins=20,range=(120,130),density=True,label=\"Generated\")\n",
    "\n",
    "\n",
    "# Get the x values for pdf calculation\n",
    "x = (binedge[0:-1]+binedge[1:])/2\n",
    "\n",
    "# Take note how you draw the fitted pdf \n",
    "plt.plot(x, norm.pdf(x,mean,std), 'r-',lw=5, alpha=0.6, label='Fitted pdf')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Fraction of entries')\n",
    "plt.legend()\n",
    "\n",
    "plt.text(mean,np.max(bincontent)*0.1,'Fitted mean = {:4.3f}'.format(mean))\n",
    "plt.text(mean,np.max(bincontent)*0.05,'Fitted sigma = {:4.3f}'.format(std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0db48ed",
   "metadata": {},
   "source": [
    "**Calculate the maximized log likelihood value for this given fit.**\n",
    "- Hint: scipy.stats.norm has the logpdf method  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded97e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "maxLL = \n",
    "print('The maximized log likelihood value is {:4.4f}'.format(maxLL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8857ad9",
   "metadata": {},
   "source": [
    "## curve_fit\n",
    "scipy.optimize provides a curve_fit method, which is based a least-squares method. In a least-squares fit, the value of $\\sum_{i}(f(x_i,\\theta) - y_i)^2$ is minimized. Here $x_i, y_i$ are parameters that define the data points, and $f(x_i,\\theta)$ is the fit function, and $\\theta$ represents a set of parameters that are *variable* in the fit.  \n",
    "\n",
    "Consult \n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html\n",
    "for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3057a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946957c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a gaussian function\n",
    "def gaussian(x,mean,std):\n",
    "    return norm.pdf(x,mean,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd314ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curve_fit returns two sets of parameters\n",
    "# the first one, a, are values of free parameters in the fit\n",
    "# the second one, b, are covariance of the fit paramters\n",
    "a,b = curve_fit(gaussian,x,bincontent,bounds=([120,1.0],[130,5]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15572e",
   "metadata": {},
   "source": [
    "Visualize the fit result. Here you will see the fit results are different from the rv_continuous.fit, because the figure of merit used in the minimization is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a,b)\n",
    "\n",
    "plt.scatter(x,bincontent,label=\"Generated Data\")\n",
    "plt.plot(x,gaussian(x,*a),'g--', label=\"Scipy curve_fit result\")\n",
    "plt.text(123,np.max(bincontent)*0.1,'Fitted mean = {:4.3f}'.format(a[0]))\n",
    "plt.text(123,np.max(bincontent)*0.05,'Fitted sigma = {:4.3f}'.format(a[1]))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Fraction of entries')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9749924",
   "metadata": {},
   "source": [
    "## Binned Log likelihood fit\n",
    "\n",
    "Here we develop code to perform a binned log likelihood fit. \n",
    "\n",
    "Earlier we used this line to produce a histogram of the random distribution\n",
    "\n",
    "`bincontent, binedge, others = plt.hist(data,bins=20,range=(120,130),density=True,label=\"Generated\")`\n",
    "\n",
    "This histogram has 20 bins between 120 and 130. Each bin has a width of 0.5. The number of events in the bins are given by the numpy array `bincontent`, and the centers of the bin can be derived from the numpy array `binedge` (you should have seen my line doing just this earlier in the notebook.)\n",
    "\n",
    "The expected number of events in a bin $i$ is $\\lambda = \\mathrm{normal}(x_i,\\mu,\\sigma)\\cdot w_i$, where $\\mathrm{normal}(x_i,\\mu,\\sigma)$ is the prediction of a normal function with a mean of $\\mu$ and a standard deviation of $\\sigma$ at the value of $x_i$, and $w_i$ is the bin width.\n",
    "\n",
    "The observed number of events is given by `bincontent`.\n",
    "\n",
    "In this exercise:\n",
    "- Write a negative log likelihood function for this fit. This function should have \n",
    "    - a numpy array named as `par` that has the mean and standard deviation of the normal distribution as its elements. The par parameters will be floating in the fit, i.e., the fit will tune these parameters\n",
    "    - a numpy array x that corresponds to the bin ceneters where the expectation is evaluated\n",
    "    - a numpy array y that corresponds to the observed number counts in the bins\n",
    "    - a binwidth parameter that is 0.5 for this problem\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb8f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizationfunction(par,x,y,binwidth):\n",
    "\n",
    "    # Your code here\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac99d71",
   "metadata": {},
   "source": [
    "- Use the scipy.optimize.minimize method to determine the mean and standard deviation \n",
    "    - think about what should be the function to optimize, what values should be given as x0, if you need to give anything to args=?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d01a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize code\n",
    "result = minimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17935213",
   "metadata": {},
   "source": [
    "Overlay the fitted PDf with the random data. Your output should look like this one\n",
    "<img src=\"https://portal.nersc.gov/project/m3438/physics77/WS08/fig3.png\" width =500>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your plotting code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1197a9bb",
   "metadata": {},
   "source": [
    "Congrats for completing the workshop!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
